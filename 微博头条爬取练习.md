## 《用Python玩转数据》项目--Sina Weibo 头条信息内容挖掘
### 一、背景
&nbsp;&nbsp;新浪微博是目前中国最大的社交平台之一，其影响力巨大，而微博头条更能反映一段时间内中国的热点事件，故通过挖掘一段时间内的微博头条标题，能从某种程度上反映出某件事情的热度。新浪微博头条示例如下：<br/>
![头条](./title.jpeg)
### 二、算法
1. 从Sina微博爬取头条数据
    * 利用Requests库爬取头条数据，在分析过程中发现，微博头条数据每次只展示部分数据，在向下滚动的页面最底部时才会再向服务器请求部分数据。如图，展示了页面数据的加载过程：<br/>
    ![](/Users/wjy/Desktop/2.jpg)<br/>
    循环构造url，不断向服务器请求，就可以获得微博头条数据。
    * 微博服务器返回的数据为json格式，其中`code`为`100000`，可以利用这个条件作为依据判断请求是否成功，`data`为所返回的数据，为`html`格式，可以利用正则表达式来解析，获取所有的头条标题。
    * 将获取的标题存入文件中
2. 标题分词
利用jieba做分词
3. 去除停用词
4. 选择名词（名称）
5. 根据词频画出词云
### 三、参考代码
``` python
import jieba.posseg as pseg
import matplotlib.pyplot as plt
import os
from wordcloud import WordCloud
import time
from scipy.misc import imread
import re
import requests


def get_weibo_toutiao():
    # titles = []  # 返回的title列表
    BASE_URL = 'https://weibo.com/a/aj/transform/loadingmoreunlogin?\
    ajwvr=6&category=1760&page={}'
    HEADERS = {
        'Host': 'weibo.com',
        'Referer': 'https://weibo.com/?category=1760',
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 Safari/537.36'
    }
    patt = re.compile(r'class="S_txt1" target="_blank">(.*?)</a>')
    with open('titles.txt', 'w') as fw:
        for i in range(200):
            r = requests.get(BASE_URL.format(i), headers=HEADERS)
            result = r.json()  # 获取返回来的json格式数据
            data = result['data']
            title = patt.findall(data)
            for t in title:
                fw.write(t + '\n')
            # titles.extend(title)
            # time.sleep(2)


def extract_words():
    with open('titles.txt','r') as fr:
        titles = fr.readlines()

    stop_words = set(line.strip() for line in open('stopwords.txt', 'r'))
    title_list = []  # 头条标题词汇列表
    name_list = []  # 人名头条列表
    for title in titles:
        if title.isspace():
            continue
        word_list = pseg.cut(title)
        for word, flag in word_list:
            if not word in stop_words:
                if flag == 'n':  # 名词
                    title_list.append(word)
                if flag == 'nr':  # 人名
                    name_list.append(word)
    d = os.path.dirname(__file__)
    mask_image1 = imread(os.path.join(d, 'background.png'))
    content1 = ' '.join(title_list)
    mask_image2 = imread(os.path.join(d, 'background.png'))
    content2 = ' '.join(name_list)
    wordcloud1 = WordCloud(font_path='PingFang.ttc', background_color='grey', mask=mask_image1, max_words=100).generate(content1)
    wordcloud2 = WordCloud(font_path='PingFang.ttc', background_color='grey', mask=mask_image2, max_words=100).generate(content2)
    plt.imshow(wordcloud1)
    plt.axis('off')
    wordcloud1.to_file('worldcloud1.jpg')
    plt.show()
    plt.imshow(wordcloud2)
    plt.axis('off')
    wordcloud2.to_file('worldcloud2.jpg')
    plt.show()


if __name__ == '__main__':
    get_weibo_toutiao()
    extract_words()
```
### 四、结果及分析
##### 微博头条中所有词汇的词云图如下：<br/>
![名词](./wc1.jpg)
##### 微博头条中姓名词云图如下：<br/>
![名词](./wc2.jpg)<br/>
图中可以看出，由于本项目中并没有对词汇进行处理，仅仅依赖于jieba的默认分词规则，所以词性并不是十分准确。还需进一步深入学习jieba等中文分词工具。但也能看出部分规律：刘若英由于最近指导的新电影上映，故热度较高。
